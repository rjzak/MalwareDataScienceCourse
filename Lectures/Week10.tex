\documentclass{beamer}
\usepackage{graphicx}
\usepackage{listings} % Syntax highlighing
\usepackage{fancyvrb} % Inline verbatim
\usepackage{hyperref} % Hyperlinks
\hypersetup{pdfpagemode=FullScreen}
\usepackage{pgfplots}
\usepackage{tikz}

\usetheme{Boadilla}
\title{Malware N-Grams}
\author{UMBC Malware Data Science}
\date{Week 10: 6 April 2020}

\begin{document}

\begin{frame}{Recap}
    Up until now, we've looked at domain-specific features. Let's shift our focus to $n$-grams.
\end{frame}

\begin{frame}{NGrams}
    \only<1>{\input{TexIncludes/ngrams.tex}}
    \only<2>{
        $N$-grams are popular with the NLP (natural language processing) community, but they are useful for malware, and other data which is variable length and doesn't fit well into a table.
        \\ ~~ \\
        This technique is what allows us to create tools which will later work on any file type, PE32, PDF, ELF, Mach-O, etc.
    }
    \only<3>{
        Something to avoid, close or adjacent ngrams, since they don't add any new information or value.
        \\ ~~ \\
        Consider the sequence \texttt{00 00 E8 00 00 00 0E 1F BA 0E 00 B4 09 CD}.
        \\ ~~ \\
        Do we really need all of these $n$-grams:
        \begin{itemize}
            \item \texttt{00 E8 00 00},
            \item \texttt{E8 00 00 00}, and
            \item \texttt{00 00 00 0E}?
        \end{itemize}
        No, and we can reduce them using a hash function, and doing a modulo (\%) on the hash.
    }
    \only<4>{
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            xlabel=N, ymode=log, xtick={4, 8, 16, 32, 64, 128},
            ytick={0,1e10, 1e40, 1e80, 1e200, 1e300},
            ylabel=Possible $n$-grams]
        \addplot table [y=K, x=N]{possibleNgrams.dat};
        \end{axis}
        \end{tikzpicture}
    }
\end{frame}

\begin{frame}[fragile]{Hashing N-grams}
We can use any hash function to obtain an integer from a byte sequence, but since speed is more important, use a fast hash function.
\\ ~~ \\
Most hash functions are used to validate data, or ensure strength in cryptographic functions. Neither case exists here.
\small
\begin{verbatim}
def rabinHash(ngram):
  r = 0;
  for index in range(len(ngram)):
    r = (r * 227 + ord(ngram[index])) % 1000005
  return r
\end{verbatim}
\end{frame}

\begin{frame}[fragile]{Reducing N-grams}
    With the previous function, we can reduce the amount of local/adjacent $n$-grams which are considered.
\begin{verbatim}
ngram_collection = set()
for ngram in get_ngrams(buffer):
  ngram_hash = rabinHash(ngram)
  if ngram_hash % 2 == 0:
    ngram_collection.append(ngram)
\end{verbatim}
\end{frame}

\begin{frame}{What does it mean?}
    \only<1>{
        There are two ways to look at this:
        \begin{enumerate}
            \item It doesn't matter. The algorithms identify sequences which help a machine learning algorithm make a determination. Retrain after some time increment has passed, gain potentially new $n$-grams, and the new model learns based on new data.
            \item It does matter. The $n$-grams might be picking up random bytes that don't correlate to any functionality or attributable features of the malware, so why trust that the model is \textit{actually} learning anything?
        \end{enumerate}
    }
    \only<2>{
        The answer? It depends.
        \begin{enumerate}
            \item Let the $n$-grams speak for themselves, and retrain when needed. This is more reliable than trusting a human to be better. Humans aren't good at looking at hundreds of data points, let alone thousands or millions.
            \item Domain-specific features and larger $n$-grams ($n > 16$, for example), allow for explainability. This is an important topic in machine learning.
            \item Third option (there's always a third option): Use both! Models can be built using $n$-grams with domain-specific features. Perhaps start collecting $n$-grams at the Entry Point, which is where the program's execution begins.
        \end{enumerate}
    }
\end{frame}

\begin{frame}{Lab 10}
    Lab 10: Write code to find $n$-grams for malware and goodware, and create a dataset.
    \\ ~~ \\
    The resulting dataset should be CSV with headers, such that 0 means the ngram was not found, 1 means the ngram was found, and the label at the end is 0 for goodware, 1 for malware. This will make it easier to use with \texttt{Pandas} and \texttt{sklearn} later.
    \\ ~~ \\
    How many $n$-grams should be found? What's the criteria for selecting one $n$-gram over another?
\end{frame}

\end{document}